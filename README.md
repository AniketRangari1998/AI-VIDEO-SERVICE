**ğŸ¬ AI Video Generation Service**

This repository provides a local AI backend for generating talking avatar videos and product promotion videos using open-source AI models.

It demonstrates how platforms like Arcads, Synthesia, HeyGen work internally â€” combining Text-to-Speech, Talking Head Generation, and Video Composition.

**âœ¨ What This Project Does**

âœ” Converts text into natural human-like speech
âœ” Generates talking avatar videos from a single image
âœ” Creates product promotion videos using avatar + product image
âœ” Exposes everything via REST APIs
âœ” Runs locally on CPU (slow but functional)
âœ” Designed to be moved to GPU/cloud later

**ğŸ§  AI Services Used (Important)**
**ğŸ¤ Text-to-Speech (TTS)**

Service: Microsoft Edge Neural TTS

Library: edge-tts

Voice Used: en-IN-PrabhatNeural

Provider: Microsoft (Edge / Azure Speech)

Cost: âœ… Free (via Edge TTS)

PrabhatNeural is a high-quality Indian English male voice provided by Microsoft.

ğŸ“Œ We do not host any TTS model ourselves â€” we call Edge TTS locally.

**ğŸ§‘ Talking Avatar Video**

Model: SadTalker

Type: Audio-driven talking face generation

Input: Image + Audio

Output: MP4 talking avatar video

ğŸ”— SadTalker GitHub
ğŸ‘‰ https://github.com/vinthony/SadTalker

ğŸ™ Huge thanks to the SadTalker authors for their incredible open-source work.
This project would not be possible without them.

**ğŸ Video & Audio Processing**

Tool: FFmpeg

Used for:

Audio conversion (MP3 â†’ WAV)

Video processing

Required on all OS

ğŸ”— https://ffmpeg.org

**ğŸ— Project Structure**
<img width="565" height="484" alt="image" src="https://github.com/user-attachments/assets/2f24af6f-bf1d-4aa9-920b-57af0f8179cb" />


**âš™ï¸ System Requirements**
Minimum (Development)

RAM: 8 GB

Disk: 20 GB free

Python: 3.10.x

CPU: Any modern CPU

âš ï¸ CPU mode is slow (1â€“5 min per video).
For production quality â†’ GPU is required.

**ğŸ Python Environment Setup (All OS)**
1ï¸âƒ£ Install Python

Download from:
ğŸ‘‰ https://www.python.org/downloads/

Verify:

python --version

2ï¸âƒ£ Create Virtual Environment
python -m venv venv


**Activate:**

Windows

venv\Scripts\activate


Linux / macOS

source venv/bin/activate

ğŸ“¦ Install Dependencies (requirements.txt)
Step 1: Install PyTorch (CPU only)

âš ï¸ Required before installing other packages

pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 \
 --index-url https://download.pytorch.org/whl/cpu

Step 2: Install all remaining dependencies
pip install -r requirements.txt

Step 3: Verify installation
python -c "import torch; print(torch.__version__)"

ğŸ FFmpeg Installation (OS-Specific)
ğŸªŸ Windows
winget install "FFmpeg (Essentials Build)"


Restart terminal and verify:

ffmpeg -version
ffprobe -version

ğŸ§ Linux (Ubuntu/Debian)
sudo apt update
sudo apt install ffmpeg -y

ğŸ macOS
brew install ffmpeg

**ğŸ¤– SadTalker Manual Setup (Required)**

SadTalker must be installed manually.

**1ï¸âƒ£ Clone SadTalker**
cd models
git clone https://github.com/vinthony/SadTalker.git

**2ï¸âƒ£ Download Pretrained Models**

Download from:
ğŸ‘‰ https://github.com/OpenTalker/SadTalker/blob/cd4c0465ae0b54a6f85af57f5c65fec9fe23e7f8/scripts/download_models.sh

Required files:

Direct download links, download and paste them in the folder -> models/SadTalker/checkpoints
SadTalker_V0.0.2_256.safetensors - https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/SadTalker_V0.0.2_256.safetensors
SadTalker_V0.0.2_512.safetensors - https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/SadTalker_V0.0.2_512.safetensors
mapping_00109-model.pth.tar - https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/mapping_00109-model.pth.tar
mapping_00229-model.pth.tar - https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/mapping_00229-model.pth.tar

Dowload weights, and paste them in the folder -> models/gfpan/weights
https://github.com/xinntao/facexlib/releases/download/v0.1.0/alignment_WFLW_4HG.pth
https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth
https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth


**3ï¸âƒ£ Verify SadTalker**
python models/SadTalker/inference.py --help


If help appears â†’ setup is correct âœ…

**â–¶ï¸ Start the Application**
uvicorn app.main:app --reload --port 9000


**Open:**

Use Postman or any other tool.

ğŸ”Œ API Endpoints (Detailed)
**0ï¸âƒ£ Health Check**

GET http://localhost:9000/health

Checks if backend is running.

Response:

{ "status": "UP" }

**ğŸ¤ 1ï¸âƒ£ Generate Audio (Text â†’ Speech)**

POST http://localhost:9000/tts

What it does

Calls Microsoft Edge Neural TTS

Generates MP3 audio

Body:

{
  "text": "This protein shaker is perfect for gym lovers",
  "voice": "en-IN-PrabhatNeural"
}


Response:

{
  "audio_path": "output/audio.mp3"
}

**ğŸ§‘ 2ï¸âƒ£ Generate Talking Avatar Video**

POST http://localhost:9000/avatar

What it does

Converts audio to WAV

Runs SadTalker

Generates talking head video

Body:

{
  "avatar_image": "uploads/avatar.jpg",
  "audio_path": "output/audio.mp3"
}


Response:

{
  "avatar_video": "output/avatar_video.mp4"
}


â³ CPU time: 1â€“5 minutes

**ğŸ› 3ï¸âƒ£ Generate Product Promotion Video**

POST http://localhost:9000/compose

What it does

Overlays product image

Creates final promotional video

Body:

{
  "avatar_video": "output/avatar_video.mp4",
  "product_image": "uploads/product.png"
}


Response:

{
  "final_video": "output/final_promo.mp4"
}

**ğŸ”„ Typical Workflow**
POST /tts
   â†“
POST /avatar
   â†“
POST /compose

âš ï¸ Performance Notes

CPU mode is slow

First run loads models (longer)

For production:

Use GPU

Disable CPU-only mode

Deploy SadTalker separately



**ğŸ™Œ Credits**

SadTalker â€“ https://github.com/vinthony/SadTalker

Microsoft Edge TTS

FFmpeg

This project builds on amazing open-source contributions â¤ï¸

**ğŸ“„ License & Disclaimer**

This project is for educational & development purposes

Check licenses of:

SadTalker

FFmpeg

Microsoft TTS

**ğŸ Final Note**

This repository shows how real AI video platforms are engineered â€”
from orchestration to ML integration to scaling.

If you can run this locally, you can run it in production.
